{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.widgets import MultiCursor\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the quality : 5.6360225140712945\n",
      "Standard deviation of the quality : 0.8075694397347023\n",
      "Median of the quality : 6.0\n"
     ]
    }
   ],
   "source": [
    "plt.bar(df['quality'].value_counts().index, df['quality'].value_counts().values)\n",
    "plt.show()\n",
    "print('Mean of the quality :', df['quality'].mean())\n",
    "print('Standard deviation of the quality :', df['quality'].std())\n",
    "print('Median of the quality :', df['quality'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary(data):\n",
    "    labels = data['quality']\n",
    "    \n",
    "    median = int(labels.median())\n",
    "    mean = int(labels.mean())\n",
    "\n",
    "    labels[labels < median] = 0\n",
    "    labels[labels >= median] = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize(data):\n",
    "\n",
    "    norm = lambda x, x_max, x_min: (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    for column in list(data.columns):\n",
    "        if column != \"quality\":\n",
    "\n",
    "            temp = data[column].to_list()\n",
    "            temp_max = max(temp)\n",
    "            temp_min = min(temp)\n",
    "\n",
    "            new_values = [norm(x, temp_max, temp_min) for x in temp]\n",
    "            data[column] = new_values\n",
    "    \n",
    "    return data\n",
    "\n",
    "def pipeline(data):\n",
    "\n",
    "    data = normalize(data)\n",
    "    data = make_binary(data)\n",
    "\n",
    "    train, test, val = np.split(data.sample(frac=1), [int(data.shape[0] * 0.6), int(data.shape[0] * 0.8)])\n",
    "    \n",
    "    test_features = test.drop('quality', axis=1)\n",
    "    test_labels = test['quality']\n",
    "\n",
    "    train_features = train.drop('quality', axis=1)\n",
    "    train_labels = train['quality']\n",
    "\n",
    "    val_features = val.drop('quality', axis=1)\n",
    "    val_labels = val['quality']\n",
    "\n",
    "    train_tensor = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "    test_tensor = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "    val_tensor = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
    "\n",
    "    train_tensor = train_tensor.shuffle(100).batch(20).prefetch(1)\n",
    "    test_tensor = test_tensor.shuffle(100).batch(20).prefetch(1)\n",
    "    val_tensor = val_tensor.shuffle(100).batch(20).prefetch(1)\n",
    "\n",
    "    train_tensor = train_tensor.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))\n",
    "    test_tensor = test_tensor.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))\n",
    "    val_tensor = val_tensor.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))\n",
    "\n",
    "    train_tensor = train_tensor.cache()\n",
    "    test_tensor = test_tensor.cache()\n",
    "    val_tensor = val_tensor.cache()\n",
    "\n",
    "    return train_tensor, test_tensor, val_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor, test_tensor, val_tensor = pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model \"\"\"\n",
    "\n",
    "class Dense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, activation):\n",
    "        super(Dense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape): \n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer='random_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        x = tf.matmul(inputs, self.w) + self.b\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_baseline(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model_baseline, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=64, activation=tf.nn.sigmoid)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=32, activation=tf.nn.sigmoid)\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\" Run data through network \"\"\"\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training(self, input, target, loss, optimizer):\n",
    "        \"\"\" Perform gradient descent and train variables \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def test(self, test_data, loss):\n",
    "        \"\"\" Evaluate metrics \"\"\"\n",
    "        test_accuracy = []\n",
    "        test_loss = []\n",
    "\n",
    "        for (input, target) in test_data:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "            sample_accuracy = np.round(pred, 0) == np.round(target, 0)\n",
    "            sample_accuracy = np.mean(sample_accuracy)\n",
    "\n",
    "            test_accuracy.append(np.mean(sample_accuracy))\n",
    "            test_loss.append(loss_value.numpy())\n",
    "\n",
    "        test_loss = tf.reduce_mean(test_loss)\n",
    "        test_accuracy = tf.reduce_mean(test_accuracy)\n",
    "\n",
    "        return test_loss, test_accuracy\n",
    "    \n",
    "    def validate(self, val_data, loss):\n",
    "        \"\"\" Evaluate metrics \"\"\"\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for (input, target) in val_data:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "            sample_accuracy = np.round(pred, 0) == np.round(target, 0)\n",
    "            sample_accuracy = np.mean(sample_accuracy)\n",
    "\n",
    "            val_accuracy.append(np.mean(sample_accuracy))\n",
    "            val_loss.append(loss_value.numpy())\n",
    "\n",
    "        val_loss = tf.reduce_mean(val_loss)\n",
    "        val_accuracy = tf.reduce_mean(val_accuracy)\n",
    "\n",
    "        return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_penalty(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model_penalty, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=64, activation=tf.nn.sigmoid, \\\n",
    "                                            # add penalty on layer's kernel\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), \\\n",
    "                                            # add penalty on layer's bias\n",
    "                                            bias_regularizer=tf.keras.regularizers.l2(1e-4), \\\n",
    "                                            # add penalty on layer's output\n",
    "                                            activity_regularizer=tf.keras.regularizers.l2(1e-5))\n",
    "        self.dense2 = tf.keras.layers.Dense(units=32, activation=tf.nn.sigmoid, \\\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), \\\n",
    "                                            bias_regularizer=tf.keras.regularizers.l2(1e-4), \\\n",
    "                                            activity_regularizer=tf.keras.regularizers.l2(1e-5))\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid, \\\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), \\\n",
    "                                            bias_regularizer=tf.keras.regularizers.l2(1e-4), \\\n",
    "                                            activity_regularizer=tf.keras.regularizers.l2(1e-5))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\" Run data through network \"\"\"\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training(self, input, target, loss, optimizer):\n",
    "        \"\"\" Perform gradient descent and train variables \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def test(self, test_data, loss):\n",
    "        \"\"\" Evaluate metrics \"\"\"\n",
    "        test_accuracy = []\n",
    "        test_loss = []\n",
    "\n",
    "        for (input, target) in test_data:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "            sample_accuracy = np.round(pred, 0) == np.round(target, 0)\n",
    "            sample_accuracy = np.mean(sample_accuracy)\n",
    "\n",
    "            test_accuracy.append(np.mean(sample_accuracy))\n",
    "            test_loss.append(loss_value.numpy())\n",
    "\n",
    "        test_loss = tf.reduce_mean(test_loss)\n",
    "        test_accuracy = tf.reduce_mean(test_accuracy)\n",
    "\n",
    "        return test_loss, test_accuracy\n",
    "    \n",
    "    def validate(self, val_data, loss):\n",
    "        \"\"\" Evaluate metrics \"\"\"\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for (input, target) in val_data:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "            sample_accuracy = np.round(pred, 0) == np.round(target, 0)\n",
    "            sample_accuracy = np.mean(sample_accuracy)\n",
    "\n",
    "            val_accuracy.append(np.mean(sample_accuracy))\n",
    "            val_loss.append(loss_value.numpy())\n",
    "\n",
    "        val_loss = tf.reduce_mean(val_loss)\n",
    "        val_accuracy = tf.reduce_mean(val_accuracy)\n",
    "\n",
    "        return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_penalty_and_dropout(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model_penalty_and_dropout, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=64, activation=tf.nn.sigmoid, \\\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), \\\n",
    "                                            bias_regularizer=tf.keras.regularizers.l2(1e-4), \\\n",
    "                                            activity_regularizer=tf.keras.regularizers.l2(1e-5))\n",
    "        self.dense2 = tf.keras.layers.Dense(units=32, activation=tf.nn.sigmoid, \\\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), \\\n",
    "                                            bias_regularizer=tf.keras.regularizers.l2(1e-4), \\\n",
    "                                            activity_regularizer=tf.keras.regularizers.l2(1e-5))\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid, \\\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), \\\n",
    "                                            bias_regularizer=tf.keras.regularizers.l2(1e-4), \\\n",
    "                                            activity_regularizer=tf.keras.regularizers.l2(1e-5))\n",
    "        # dropout layer, rate is likelihood that neurons activation is set to zero during training\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(rate=0.2)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\" Run data through network \"\"\"\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout_layer(x, training=True)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training(self, input, target, loss, optimizer):\n",
    "        \"\"\" Perform gradient descent and train variables \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def test(self, test_data, loss):\n",
    "        \"\"\" Evaluate metrics \"\"\"\n",
    "        test_accuracy = []\n",
    "        test_loss = []\n",
    "\n",
    "        for (input, target) in test_data:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "            sample_accuracy = np.round(pred, 0) == np.round(target, 0)\n",
    "            sample_accuracy = np.mean(sample_accuracy)\n",
    "\n",
    "            test_accuracy.append(np.mean(sample_accuracy))\n",
    "            test_loss.append(loss_value.numpy())\n",
    "\n",
    "        test_loss = tf.reduce_mean(test_loss)\n",
    "        test_accuracy = tf.reduce_mean(test_accuracy)\n",
    "\n",
    "        return test_loss, test_accuracy\n",
    "    \n",
    "    def validate(self, val_data, loss):\n",
    "        \"\"\" Evaluate metrics \"\"\"\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for (input, target) in val_data:\n",
    "            pred = self.call(input)\n",
    "            loss_value = loss(target, pred)\n",
    "\n",
    "            sample_accuracy = np.round(pred, 0) == np.round(target, 0)\n",
    "            sample_accuracy = np.mean(sample_accuracy)\n",
    "\n",
    "            val_accuracy.append(np.mean(sample_accuracy))\n",
    "            val_loss.append(loss_value.numpy())\n",
    "\n",
    "        val_loss = tf.reduce_mean(val_loss)\n",
    "        val_accuracy = tf.reduce_mean(val_accuracy)\n",
    "\n",
    "        return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_baseline()\n",
    "model.build(input_shape=(20,64))\n",
    "model.compile()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(optimizer, epochs, penalty=False, dropout=False):\n",
    "    \"\"\" External training loop \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    if(penalty and dropout):\n",
    "        model = Model_penalty_and_dropout()\n",
    "    elif(penalty and not(dropout)):\n",
    "        model = Model_penalty()\n",
    "    else:\n",
    "        model = Model_baseline()\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    optimizer = optimizer(learning_rate=0.1)\n",
    "\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    # Test runthrough\n",
    "    test_loss, test_accuracy = model.test(test_tensor, loss)\n",
    "\n",
    "    accuracies.append(test_accuracy)\n",
    "    losses.append(test_loss)\n",
    "\n",
    "    # Train runthrough\n",
    "    train_loss, train_accuracy = model.test(train_tensor, loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation runthrough\n",
    "    val_loss, val_accuracy = model.validate(val_tensor, loss)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print('Optimizer: ',optimizer)\n",
    "    print('Initial loss:', test_loss.numpy(), 'Initial accuracy:', test_accuracy.numpy(),'\\n')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch: {epoch}, accuracy of {accuracies[-1]}')\n",
    "\n",
    "      epoch_loss = []\n",
    "\n",
    "      for (input, target) in train_tensor:\n",
    "          loss_value = model.training(input, target, loss, optimizer)\n",
    "          epoch_loss.append(loss_value)\n",
    "\n",
    "      train_losses.append(tf.reduce_mean(epoch_loss))\n",
    "      train_accuracies.append(model.test(train_tensor, loss)[1])\n",
    "\n",
    "      val_loss, val_accuracy = model.validate(val_tensor, loss)\n",
    "      val_accuracies.append(val_accuracy)\n",
    "      val_losses.append(val_loss)\n",
    "\n",
    "      test_loss, test_accuracy = model.test(test_tensor, loss)\n",
    "      accuracies.append(test_accuracy)\n",
    "      losses.append(test_loss)\n",
    "\n",
    "    print('\\nFinal loss:', test_loss.numpy(), 'Final accuracy:', test_accuracy.numpy(),'\\n')\n",
    "\n",
    "    return accuracies, losses, train_accuracies, train_losses, val_accuracies, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = training_loop(tf.keras.optimizers.SGD, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_penalty = training_loop(tf.keras.optimizers.SGD, epochs=10, penalty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_penalty_dropout = training_loop(tf.keras.optimizers.SGD, epochs=10, penalty=True, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = training_loop(tf.keras.optimizers.Adam, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_penalty = training_loop(tf.keras.optimizers.Adam, epochs=10, penalty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_penalty_dropout = training_loop(tf.keras.optimizers.Adam, epochs=10, penalty=True, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = training_loop(tf.keras.optimizers.Adagrad, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_penalty = training_loop(tf.keras.optimizers.Adagrad, epochs=10, penalty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_penalty_dropout = training_loop(tf.keras.optimizers.Adagrad, epochs=10, penalty=True, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta = training_loop(tf.keras.optimizers.Adadelta, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta_penalty = training_loop(tf.keras.optimizers.Adadelta, epochs=10, penalty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta_penalty_dropout = training_loop(tf.keras.optimizers.Adadelta, epochs=10, penalty=True, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = training_loop(tf.keras.optimizers.RMSprop, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_penalty = training_loop(tf.keras.optimizers.RMSprop, epochs=10, penalty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_penalty_dropout = training_loop(tf.keras.optimizers.RMSprop, epochs=10, penalty=True, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Comparing via plot \"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(13,20), dpi=150)\n",
    "gs = fig.add_gridspec(15,hspace=0)\n",
    "axs = gs.subplots(sharex=True)\n",
    "\n",
    "cursor = MultiCursor(fig.canvas,(axs[0],axs[1], axs[2], axs[3], axs[4]),horizOn=False,vertOn=True,color=\"black\",linewidth=1.0)\n",
    "\n",
    "axs[0].plot(np.arange(0,len(sgd[0])),sgd[0],label='Test Accuracy')\n",
    "axs[0].plot(np.arange(0,len(sgd[1])),sgd[1],label='Test Loss')\n",
    "axs[0].plot(np.arange(0,len(sgd[2])),sgd[2],label='Train accuracy')\n",
    "axs[0].plot(np.arange(0,len(sgd[3])),sgd[3],label='Train loss')\n",
    "axs[0].plot(np.arange(0,len(sgd[4])),sgd[4],label='Validation accuracy')\n",
    "axs[0].plot(np.arange(0,len(sgd[5])),sgd[5],label='Validation loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Accuracy/Loss')\n",
    "axs[0].legend(loc='center right')\n",
    "axs[0].annotate('SGD',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[1].plot(np.arange(0,len(sgd_penalty[0])),sgd_penalty[0],label='Test Accuracy')\n",
    "axs[1].plot(np.arange(0,len(sgd_penalty[1])),sgd_penalty[1],label='Test Loss')\n",
    "axs[1].plot(np.arange(0,len(sgd_penalty[2])),sgd_penalty[2],label='Train accuracy')\n",
    "axs[1].plot(np.arange(0,len(sgd_penalty[3])),sgd_penalty[3],label='Train loss')\n",
    "axs[1].plot(np.arange(0,len(sgd_penalty[4])),sgd_penalty[4],label='Validation accuracy')\n",
    "axs[1].plot(np.arange(0,len(sgd_penalty[5])),sgd_penalty[5],label='Validation loss')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy/Loss')\n",
    "axs[1].legend(loc='center right')\n",
    "axs[1].annotate('SGD with penalty',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[2].plot(np.arange(0,len(sgd_penalty_dropout[0])),sgd_penalty_dropout[0],label='Test Accuracy')\n",
    "axs[2].plot(np.arange(0,len(sgd_penalty_dropout[1])),sgd_penalty_dropout[1],label='Test Loss')\n",
    "axs[2].plot(np.arange(0,len(sgd_penalty_dropout[2])),sgd_penalty_dropout[2],label='Train accuracy')\n",
    "axs[2].plot(np.arange(0,len(sgd_penalty_dropout[3])),sgd_penalty_dropout[3],label='Train loss')\n",
    "axs[2].plot(np.arange(0,len(sgd_penalty_dropout[4])),sgd_penalty_dropout[4],label='Validation accuracy')\n",
    "axs[2].plot(np.arange(0,len(sgd_penalty_dropout[5])),sgd_penalty_dropout[5],label='Validation loss')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Accuracy/Loss')\n",
    "axs[2].legend(loc='center right')\n",
    "axs[2].annotate('SGD with penalty and dropout',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[3].plot(np.arange(0,len(adam[0])),adam[0],label='Test Accuracy')\n",
    "axs[3].plot(np.arange(0,len(adam[1])),adam[1],label='Test Loss')\n",
    "axs[3].plot(np.arange(0,len(adam[2])),adam[2],label='Train accuracy')\n",
    "axs[3].plot(np.arange(0,len(adam[3])),adam[3],label='Train loss')\n",
    "axs[3].plot(np.arange(0,len(adam[4])),adam[4],label='Validation accuracy')\n",
    "axs[3].plot(np.arange(0,len(adam[5])),adam[5],label='Validation loss')\n",
    "axs[3].set_xlabel('Epochs')\n",
    "axs[3].set_ylabel('Accuracy/Loss')\n",
    "axs[3].legend(loc='center right')\n",
    "axs[3].annotate('Adam',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[4].plot(np.arange(0,len(adam_penalty[0])),adam_penalty[0],label='Test Accuracy')\n",
    "axs[4].plot(np.arange(0,len(adam_penalty[1])),adam_penalty[1],label='Test Loss')\n",
    "axs[4].plot(np.arange(0,len(adam_penalty[2])),adam_penalty[2],label='Train accuracy')\n",
    "axs[4].plot(np.arange(0,len(adam_penalty[3])),adam_penalty[3],label='Train loss')\n",
    "axs[4].plot(np.arange(0,len(adam_penalty[4])),adam_penalty[4],label='Validation accuracy')\n",
    "axs[4].plot(np.arange(0,len(adam_penalty[5])),adam_penalty[5],label='Validation loss')\n",
    "axs[4].set_xlabel('Epochs')\n",
    "axs[4].set_ylabel('Accuracy/Loss')\n",
    "axs[4].legend(loc='center right')\n",
    "axs[4].annotate('Adam with penalty',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[5].plot(np.arange(0,len(adam_penalty_dropout[0])),adam_penalty_dropout[0],label='Test Accuracy')\n",
    "axs[5].plot(np.arange(0,len(adam_penalty_dropout[1])),adam_penalty_dropout[1],label='Test Loss')\n",
    "axs[5].plot(np.arange(0,len(adam_penalty_dropout[2])),adam_penalty_dropout[2],label='Train accuracy')\n",
    "axs[5].plot(np.arange(0,len(adam_penalty_dropout[3])),adam_penalty_dropout[3],label='Train loss')\n",
    "axs[5].plot(np.arange(0,len(adam_penalty_dropout[4])),adam_penalty_dropout[4],label='Validation accuracy')\n",
    "axs[5].plot(np.arange(0,len(adam_penalty_dropout[5])),adam_penalty_dropout[5],label='Validation loss')\n",
    "axs[5].set_xlabel('Epochs')\n",
    "axs[5].set_ylabel('Accuracy/Loss')\n",
    "axs[5].legend(loc='center right')\n",
    "axs[5].annotate('Adam with penalty and dropout',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[6].plot(np.arange(0,len(adagrad[0])),adagrad[0],label='Test Accuracy')\n",
    "axs[6].plot(np.arange(0,len(adagrad[1])),adagrad[1],label='Test Loss')\n",
    "axs[6].plot(np.arange(0,len(adagrad[2])),adagrad[2],label='Train accuracy')\n",
    "axs[6].plot(np.arange(0,len(adagrad[3])),adagrad[3],label='Train loss')\n",
    "axs[6].set_xlabel('Epochs')\n",
    "axs[6].set_ylabel('Accuracy/Loss')\n",
    "axs[6].legend(loc='center right')\n",
    "axs[6].annotate('Adagrad',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[7].plot(np.arange(0,len(adagrad_penalty[0])),adagrad_penalty[0],label='Test Accuracy')\n",
    "axs[7].plot(np.arange(0,len(adagrad_penalty[1])),adagrad_penalty[1],label='Test Loss')\n",
    "axs[7].plot(np.arange(0,len(adagrad_penalty[2])),adagrad_penalty[2],label='Train accuracy')\n",
    "axs[7].plot(np.arange(0,len(adagrad_penalty[3])),adagrad_penalty[3],label='Train loss')\n",
    "axs[7].set_xlabel('Epochs')\n",
    "axs[7].set_ylabel('Accuracy/Loss')\n",
    "axs[7].legend(loc='center right')\n",
    "axs[7].annotate('Adagrad with penalty',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[8].plot(np.arange(0,len(adagrad_penalty_dropout[0])),adagrad_penalty_dropout[0],label='Test Accuracy')\n",
    "axs[8].plot(np.arange(0,len(adagrad_penalty_dropout[1])),adagrad_penalty_dropout[1],label='Test Loss')\n",
    "axs[8].plot(np.arange(0,len(adagrad_penalty_dropout[2])),adagrad_penalty_dropout[2],label='Train accuracy')\n",
    "axs[8].plot(np.arange(0,len(adagrad_penalty_dropout[3])),adagrad_penalty_dropout[3],label='Train loss')\n",
    "axs[8].set_xlabel('Epochs')\n",
    "axs[8].set_ylabel('Accuracy/Loss')\n",
    "axs[8].legend(loc='center right')\n",
    "axs[8].annotate('Adagrad with penalty and dropout',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[9].plot(np.arange(0,len(adadelta[0])),adadelta[0],label='Test Accuracy')\n",
    "axs[9].plot(np.arange(0,len(adadelta[1])),adadelta[1],label='Test Loss')\n",
    "axs[9].plot(np.arange(0,len(adadelta[2])),adadelta[2],label='Train accuracy')\n",
    "axs[9].plot(np.arange(0,len(adadelta[3])),adadelta[3],label='Train loss')\n",
    "axs[9].set_xlabel('Epochs')\n",
    "axs[9].set_ylabel('Accuracy/Loss')\n",
    "axs[9].legend(loc='center right')\n",
    "axs[9].annotate('Adadelta',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[10].plot(np.arange(0,len(adadelta_penalty[0])),adadelta_penalty[0],label='Test Accuracy')\n",
    "axs[10].plot(np.arange(0,len(adadelta_penalty[1])),adadelta_penalty[1],label='Test Loss')\n",
    "axs[10].plot(np.arange(0,len(adadelta_penalty[2])),adadelta_penalty[2],label='Train accuracy')\n",
    "axs[10].plot(np.arange(0,len(adadelta_penalty[3])),adadelta_penalty[3],label='Train loss')\n",
    "axs[10].set_xlabel('Epochs')\n",
    "axs[10].set_ylabel('Accuracy/Loss')\n",
    "axs[10].legend(loc='center right')\n",
    "axs[10].annotate('Adadelta with penalty',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[11].plot(np.arange(0,len(adadelta_penalty_dropout[0])),adadelta_penalty_dropout[0],label='Test Accuracy')\n",
    "axs[11].plot(np.arange(0,len(adadelta_penalty_dropout[1])),adadelta_penalty_dropout[1],label='Test Loss')\n",
    "axs[11].plot(np.arange(0,len(adadelta_penalty_dropout[2])),adadelta_penalty_dropout[2],label='Train accuracy')\n",
    "axs[11].plot(np.arange(0,len(adadelta_penalty_dropout[3])),adadelta_penalty_dropout[3],label='Train loss')\n",
    "axs[11].set_xlabel('Epochs')\n",
    "axs[11].set_ylabel('Accuracy/Loss')\n",
    "axs[11].legend(loc='center right')\n",
    "axs[11].annotate('Adadelta with penalty and dropout',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[12].plot(np.arange(0,len(rmsprop[0])),rmsprop[0],label='Test Accuracy')\n",
    "axs[12].plot(np.arange(0,len(rmsprop[1])),rmsprop[1],label='Test Loss')\n",
    "axs[12].plot(np.arange(0,len(rmsprop[2])),rmsprop[2],label='Train accuracy')\n",
    "axs[12].plot(np.arange(0,len(rmsprop[3])),rmsprop[3],label='Train loss')\n",
    "axs[12].set_xlabel('Epochs')\n",
    "axs[12].set_ylabel('Accuracy/Loss')\n",
    "axs[12].legend(loc='center right')\n",
    "axs[12].annotate('RMSprop',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[13].plot(np.arange(0,len(rmsprop_penalty[0])),rmsprop_penalty[0],label='Test Accuracy')\n",
    "axs[13].plot(np.arange(0,len(rmsprop_penalty[1])),rmsprop_penalty[1],label='Test Loss')\n",
    "axs[13].plot(np.arange(0,len(rmsprop_penalty[2])),rmsprop_penalty[2],label='Train accuracy')\n",
    "axs[13].plot(np.arange(0,len(rmsprop_penalty[3])),rmsprop_penalty[3],label='Train loss')\n",
    "axs[13].set_xlabel('Epochs')\n",
    "axs[13].set_ylabel('Accuracy/Loss')\n",
    "axs[13].legend(loc='center right')\n",
    "axs[13].annotate('RMSprop with penalty',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "axs[14].plot(np.arange(0,len(rmsprop_penalty_dropout[0])),rmsprop_penalty_dropout[0],label='Test Accuracy')\n",
    "axs[14].plot(np.arange(0,len(rmsprop_penalty_dropout[1])),rmsprop_penalty_dropout[1],label='Test Loss')\n",
    "axs[14].plot(np.arange(0,len(rmsprop_penalty_dropout[2])),rmsprop_penalty_dropout[2],label='Train accuracy')\n",
    "axs[14].plot(np.arange(0,len(rmsprop_penalty_dropout[3])),rmsprop_penalty_dropout[3],label='Train loss')\n",
    "axs[14].set_xlabel('Epochs')\n",
    "axs[14].set_ylabel('Accuracy/Loss')\n",
    "axs[14].legend(loc='center right')\n",
    "axs[14].annotate('RMSprop with penalty and dropout',xy=(0.8,0.5),xytext=(0.85,0.65),xycoords='axes fraction',fontsize=10)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization methods:**\n",
    "- change number of units and layers\n",
    "- use different optimizers (SGD, Adam, Adagrad, RMSProp, Adadelta)\n",
    "- normalize the input data\n",
    "- adjust the batch size (the larger the batch size, the more accurate each training step because more examples are taken into consideration)\n",
    "- use regularization methods (penalty, dropout)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
