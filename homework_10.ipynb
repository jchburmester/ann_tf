{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBhSfePf7/yPcl7iyqdoVy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchburmester/ann_tf/blob/main/homework_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 10**"
      ],
      "metadata": {
        "id": "5Q_mOcML6eZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "p-D8UMCtNdZd"
      },
      "outputs": [],
      "source": [
        "\"\"\"Cell for imports & variable settings\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import tqdm\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Reading data and print some examples\"\"\"\n",
        "\n",
        "with open('bible.txt') as file:\n",
        "    lines = file.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSPn7hni1W2o",
        "outputId": "83085e01-4298-480a-fb1d-d811310a738d"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth.\n",
            "\n",
            "1:2 And the earth was without form, and void; and darkness was upon\n",
            "the face of the deep. And the Spirit of God moved upon the face of the\n",
            "waters.\n",
            "\n",
            "1:3 And God said, Let there be light: and there was light.\n",
            "\n",
            "1:4 And God saw the light, that it was good: and God divided the light\n",
            "from the darkness.\n",
            "\n",
            "1:5 And God called the light Day, and the darkness he called Night.\n",
            "And the evening and the morning were the first day.\n",
            "\n",
            "1:6 And God said, Let there be a firmament in the midst of the waters,\n",
            "and let it divide the waters from the waters.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "JDxVJK_97xF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Cell for generating training data\"\"\"\n",
        "  \n",
        "def gen_train(sequences, window_size, num_ns, vocab_size, seed):  \n",
        "  \n",
        "  # elements of each training example are appended to these lists\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # build sampling table for vocab_size tokens\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=vocab_size)\n",
        "  \n",
        "  # iterate over all sequences (sentences) in dataset\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # generate positive skip-gram pairs for a sequence (sentence)\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "        sequence,\n",
        "        vocabulary_size=vocab_size,\n",
        "        window_size=window_size,\n",
        "        negative_samples=0)\n",
        "    \n",
        "    # iterate over each positive skip-gram pair to produce training examples\n",
        "    # with positive context word and negative samples\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=seed,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # append each element from the training example to global lists\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "metadata": {
        "id": "x7TFOmWYttR_"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Construct a TextLineDataset object\"\"\"\n",
        "\n",
        "text_ds = tf.data.TextLineDataset('bible.txt').filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "metadata": {
        "id": "ig2s8Cc_VIYk"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Cell for pre-processing\"\"\"\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase, \n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# define the vocabulary size and number of words in a sequence\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# normalize, split, and map strings to integers\n",
        "# set output_sequence_length length to pad all samples to same length\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "#  create vocabulary\n",
        "vectorize_layer.adapt(text_ds.batch(1024))\n",
        "\n",
        "# save the created vocabulary for reference\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "\n",
        "# vectorize the data in text_ds\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "\n",
        "# obtain sequences from dataset\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())"
      ],
      "metadata": {
        "id": "MeMObhEFQUX2"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Inspect some examples\"\"\"\n",
        "\n",
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfgI6LIJ4xl4",
        "outputId": "46f06e6f-2b0b-4ba7-82de-015415b3cd06"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2 219 404   4 133 162   1   0   0   0] => ['the', 'first', 'book', 'of', 'moses', 'called', '[UNK]', '', '', '']\n",
            "[1003    7    2  684   28 1424    2  173    3    2] => ['11', 'in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the']\n",
            "[1002    3    2  111   27  223 2399    3 2366    3] => ['12', 'and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and']\n",
            "[   2  230    4    2 1010    3    2  191    4   28] => ['the', 'face', 'of', 'the', 'deep', 'and', 'the', 'spirit', 'of', 'god']\n",
            "[302   0   0   0   0   0   0   0   0   0] => ['waters', '', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Generate training examples from sequences\"\"\"\n",
        "\n",
        "targets, contexts, labels = gen_train(\n",
        "    sequences=sequences,\n",
        "    window_size=2,\n",
        "    num_ns=4,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)[:,:,0]\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqil4Y1wWNkK",
        "outputId": "6c8e2c61-bdcb-458f-e422-28d8edc75cc6"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 74644/74644 [06:54<00:00, 180.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "targets.shape: (2160808,)\n",
            "contexts.shape: (2160808, 5)\n",
            "labels.shape: (2160808, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Configure dataset for performance\"\"\"\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0P_dhVs6vSm",
        "outputId": "9c01b591-ab32-4145-b3e3-4f29421645dc"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5)), (1024, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and Training"
      ],
      "metadata": {
        "id": "vr7V9bwq75WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Create a Word2Vec class using keras\"\"\"\n",
        "\n",
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                       embedding_dim,\n",
        "                                       input_length=num_ns+1)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "      \n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "\n",
        "    # context_emb: (batch, context, embed)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "    \n",
        "    # dots: (batch, context)\n",
        "    return dots"
      ],
      "metadata": {
        "id": "ySQKXn0e8Yiy"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define loss and compile model\"\"\"\n",
        "\n",
        "num_ns = 4\n",
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "\n",
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "E01xm3cn9Fd9"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Training the model\"\"\"\n",
        "\n",
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O78_cGjK9fO_",
        "outputId": "b921174b-24db-4432-9d93-98cd4eeb7aef"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2110/2110 [==============================] - 40s 19ms/step - loss: 1.3294 - accuracy: 0.4597\n",
            "Epoch 2/20\n",
            "2110/2110 [==============================] - 33s 16ms/step - loss: 1.2014 - accuracy: 0.5161\n",
            "Epoch 3/20\n",
            "2110/2110 [==============================] - 36s 17ms/step - loss: 1.1541 - accuracy: 0.5364\n",
            "Epoch 4/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.1246 - accuracy: 0.5485\n",
            "Epoch 5/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.1029 - accuracy: 0.5574\n",
            "Epoch 6/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.0854 - accuracy: 0.5641\n",
            "Epoch 7/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.0706 - accuracy: 0.5699\n",
            "Epoch 8/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.0577 - accuracy: 0.5749\n",
            "Epoch 9/20\n",
            "2110/2110 [==============================] - 33s 16ms/step - loss: 1.0463 - accuracy: 0.5790\n",
            "Epoch 10/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.0360 - accuracy: 0.5830\n",
            "Epoch 11/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.0268 - accuracy: 0.5864\n",
            "Epoch 12/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.0185 - accuracy: 0.5896\n",
            "Epoch 13/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.0110 - accuracy: 0.5923\n",
            "Epoch 14/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 1.0043 - accuracy: 0.5949\n",
            "Epoch 15/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 0.9983 - accuracy: 0.5972\n",
            "Epoch 16/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 0.9929 - accuracy: 0.5991\n",
            "Epoch 17/20\n",
            "2110/2110 [==============================] - 35s 17ms/step - loss: 0.9880 - accuracy: 0.6009\n",
            "Epoch 18/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 0.9836 - accuracy: 0.6023\n",
            "Epoch 19/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 0.9796 - accuracy: 0.6037\n",
            "Epoch 20/20\n",
            "2110/2110 [==============================] - 34s 16ms/step - loss: 0.9759 - accuracy: 0.6049\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9c20408510>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    }
  ]
}